{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YFUEePfa9nt3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHNQFNSZ90HS"
      },
      "outputs": [],
      "source": [
        "## Load the dataset\n",
        "tweets_df = pd.read_csv(\"chatgpt.csv\")\n",
        "\n",
        "#Display the dataframe's initial rows\n",
        "tweets_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Data Cleaning\n",
        "\n",
        "# Drop unwanted column (index column)\n",
        "tweets_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = tweets_df.isnull().sum()\n",
        "\n",
        "# Check for duplicates\n",
        "duplicates = tweets_df.duplicated().sum()\n",
        "\n",
        "missing_values, duplicates\n",
        "\n",
        "# Remove duplicates\n",
        "tweets_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Examine the dataframe's form after eliminating duplicates.\n",
        "tweets_df.shape"
      ],
      "metadata": {
        "id": "voi-IYfr_ufK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwPzatzd-U_k"
      },
      "outputs": [],
      "source": [
        "##Data pre-processing which involves text normalization\n",
        "#Importing Libraries and Downloading Necessary Data from NLTK:\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#Setting Up Preprocessing Utilities\n",
        "stop_words = set(stopwords.words('english'))\n",
        "st = PorterStemmer()\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "#Utility Function to Check for Alphabetic Characters\n",
        "def is_alpha(word):\n",
        "    for part in word.split('-'):\n",
        "        if not part.isalpha():\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "#Data Cleaning Function\n",
        "def clean_dataset(text):\n",
        "    text = re.sub(r'http\\S+', '', text) # removing links\n",
        "    text = re.sub(r'\\\\n', ' ', text) # removing \\\\n\n",
        "    text = re.sub(r\"\\s*#\\S+\", \"\", text) # removing hash tags\n",
        "    text = re.sub(r\"\\s*@\\S+\", \"\", text) # removing @\n",
        "    text = text.lower()\n",
        "    words = [word for word in word_tokenize(text) if is_alpha(word)]\n",
        "    #words = [st.stem(word) for word in words]\n",
        "    words = [lem.lemmatize(word) for word in words]\n",
        "\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    text = \" \".join(words)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "#Applying the Cleaning Function to the DataFrame\n",
        "tweets_df.insert(len(tweets_df.columns)-1, \"cleaned_tweets\", tweets_df['tweets'].apply(clean_dataset))\n",
        "tweets_df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmBMq6or-iHt"
      },
      "outputs": [],
      "source": [
        "##Exploratory Data Analysis\n",
        "\n",
        "#Determine the numerical column summary statistics in the given dataset.\n",
        "summary_statistics = tweets_df.describe()\n",
        "\n",
        "#Display the summary statistics\n",
        "summary_statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLvPO83L-lJa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "label_count = tweets_df['labels'].value_counts()  #Data Preparation\n",
        "fig,axes = plt.subplots(nrows=1, ncols=2, figsize=(6,4)) #Setting Up the Plot Area\n",
        "\n",
        "sns.set_theme(style='darkgrid', palette='pastel') #Configuring Seaborn Theme\n",
        "color = sns.color_palette(palette='pastel')\n",
        "\n",
        "#Create pie chart\n",
        "explode = [0.02]*len(label_count)\n",
        "axes[0].pie(label_count.values, labels=label_count.index, autopct='%1.1f%%', colors=color, explode=explode)\n",
        "axes[0].set_title('Percentage Label')\n",
        "\n",
        "# Plot countplot (bar chart)\n",
        "sns.countplot(data=tweets_df, x='labels', ax=axes[1])  # Use 'labels' column directly without converting to float\n",
        "axes[1].set_title('Count Label')\n",
        "axes[1].set_xlabel('Label')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JTrtO79-ooE"
      },
      "outputs": [],
      "source": [
        "#Tweet length calculation\n",
        "tweets_df['tweet_length'] = tweets_df['tweets'].apply(len)\n",
        "\n",
        "# Plot the distribution of tweet lengths\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(tweets_df['tweet_length'], bins=50, kde=False)\n",
        "plt.title('Distribution of Tweet Lengths')\n",
        "plt.xlabel('Length of a Tweet')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Descriptive statistics of tweet lengths\n",
        "tweet_length_stats = tweets_df['tweet_length'].describe()\n",
        "tweet_length_stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLkeFNt9-rzD"
      },
      "outputs": [],
      "source": [
        "# Determine the word count of tweets to figure out their length.\n",
        "tweets_df['tweet_length'] = tweets_df['tweets'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Determine how many characters each tweet has.\n",
        "tweets_df['char_count'] = tweets_df['tweets'].apply(len)\n",
        "\n",
        "# Show the revised dataframe with the additional columns.\n",
        "tweets_df[['tweet_length', 'char_count']].head()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the size of the figure\n",
        "plt.figure(figsize=(6,4))\n",
        "\n",
        "# KDE Plot creation for tweet length\n",
        "plt.subplot(2,1,1)\n",
        "sns.kdeplot(data=tweets_df, x='tweet_length', hue='labels', shade=True)\n",
        "plt.title('Tweet length per label type')\n",
        "plt.xlim(0, tweets_df['tweet_length'].max())  # Set the x-axis limit to the max tweet length\n",
        "\n",
        "# KDE Plot creation for character count\n",
        "plt.subplot(2,1,2)\n",
        "sns.kdeplot(data=tweets_df, x='char_count', hue='labels', shade=True)\n",
        "plt.title('Tweet character count per label type')\n",
        "plt.xlim(0, tweets_df['char_count'].max())  # Set the x-axis limit to the max character count\n",
        "\n",
        "# Display Plots\n",
        "plt.tight_layout()  # Adjust the layout\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdOfYWonCuQA"
      },
      "outputs": [],
      "source": [
        "# 4. Lexical Diversity\n",
        "# Function Definition\n",
        "def lexical_diversity(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return len(set(tokens)) / len(tokens) if tokens else 0\n",
        "\n",
        "tweets_df['lexical_diversity'] = tweets_df['tweets'].apply(lexical_diversity) #Applying Lexical Diversity Function to DataFrame\n",
        "\n",
        "average_diversity_by_sentiment = tweets_df.groupby('labels')['lexical_diversity'].mean() #Finding the Average Lexical Diversity Based on Sentiment\n",
        "\n",
        "# Plot lexical diversity by sentiment\n",
        "average_diversity_by_sentiment.plot(kind='bar', figsize=(6, 4))\n",
        "plt.title('Lexical Diversity by Sentiment')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Lexical Diversity')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Positive Word cloud\n",
        "from wordcloud import WordCloud\n",
        "# Download the stopwords from NLTK\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Now that the stopwords are downloaded, we can try creating the word cloud again\n",
        "# Set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out the positive tweets\n",
        "positive_tweets = tweets_df[tweets_df['labels'] == 'good']\n",
        "\n",
        "# Clean the positive tweets\n",
        "positive_tweets['cleaned_tweets'] = positive_tweets['tweets'].apply(clean_dataset)\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud = WordCloud(width = 800, height = 500,\n",
        "                background_color ='white',\n",
        "                stopwords = stop_words,\n",
        "                min_font_size = 10).generate(\" \".join(positive_tweets['cleaned_tweets']))\n",
        "\n",
        "# Plot the WordCloud image\n",
        "plt.figure(figsize = (6, 6), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "##Negative Word Cloud\n",
        "# Filter out the negative tweets\n",
        "negative_tweets = tweets_df[tweets_df['labels'] == 'bad']\n",
        "\n",
        "# Clean the negative_tweets\n",
        "negative_tweets['cleaned_tweets'] = negative_tweets['tweets'].apply(clean_dataset)\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud = WordCloud(width = 800, height = 500,\n",
        "                background_color ='white',\n",
        "                stopwords = stop_words,\n",
        "                min_font_size = 10).generate(\" \".join(negative_tweets['cleaned_tweets']))\n",
        "\n",
        "# Plot the WordCloud image\n",
        "plt.figure(figsize = (6, 6), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lk2tpFRq-C7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "27ivqJamE-ed"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split # For splitting the dataset into training and testing sets\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # To transform textual data into a numerical representation\n",
        "\n",
        "#  Feature Engineering - TF-IDF\n",
        "# Create a TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 words to keep the feature space manageable\n",
        "\n",
        "# Apply the TF-IDF vectorizer to the 'cleaned_tweets' column of the dataset\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(tweets_df['cleaned_tweets'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wdFMxYg7E_N4"
      },
      "outputs": [],
      "source": [
        "#  Data preparation for training\n",
        "#  Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, tweets_df['labels'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDdkuUtCFCb8"
      },
      "outputs": [],
      "source": [
        "##Logisic Regression Model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "#  Train the Logistic Regression model\n",
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "#  Predict on the testing set\n",
        "y_pred = logistic_model.predict(X_test)\n",
        "#  Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXA1GoVlhO1F"
      },
      "outputs": [],
      "source": [
        "#Confusion matrix of Logistic Regression\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Start time for training\n",
        "start_train_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# End time for training\n",
        "end_train_time = time.time()\n",
        "training_time = end_train_time - start_train_time\n",
        "\n",
        "# Start time for prediction\n",
        "start_pred_time = time.time()\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "# End time for prediction\n",
        "end_pred_time = time.time()\n",
        "prediction_time = end_pred_time - start_pred_time\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Creating a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(4, 2))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'], yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xaih5FwZG0sY"
      },
      "outputs": [],
      "source": [
        "##Multinomial Naive Bayes model\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "#  Train the Naive Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "#  Predict on the testing set\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "\n",
        "#  Evaluate the model's performance\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "print(f'Accuracy (Naive Bayes): {accuracy_nb}')\n",
        "print(classification_report(y_test, y_pred_nb))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance Evaluation\n",
        "\n",
        "#Function to evaluate performance of naive bayes model\n",
        "def evaluate_naive_bayes(model, X_test, y_test):\n",
        "    # Start the timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Predict probabilities\n",
        "    # If X_test is a sparse matrix, convert it to a dense matrix format\n",
        "    if hasattr(X_test, \"toarray\"):\n",
        "        X_test = X_test.toarray()\n",
        "    proba = model.predict_proba(X_test)\n",
        "\n",
        "    # Calculate the evaluation runtime\n",
        "    eval_runtime = time.time() - start_time\n",
        "\n",
        "    # Calculate the number of samples processed per second\n",
        "    eval_samples_per_second = X_test.shape[0] / eval_runtime\n",
        "\n",
        "    # Calculate log loss (cross-entropy loss)\n",
        "    # Convert y_test to a dense matrix format if it is sparse\n",
        "    if hasattr(y_test, \"toarray\"):\n",
        "        y_test = y_test.toarray()\n",
        "    eval_loss = log_loss(y_test, proba)\n",
        "\n",
        "\n",
        "    # Return a dictionary containing the evaluation metrics\n",
        "    return {\n",
        "        'eval_loss': eval_loss,\n",
        "        'eval_runtime': eval_runtime,\n",
        "        'eval_samples_per_second': eval_samples_per_second,\n",
        "    }\n",
        "\n",
        "# Now, evaluate the Naive Bayes model\n",
        "evaluation_results = evaluate_naive_bayes(nb_model, X_test, y_test)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(evaluation_results)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J_EWFnExl77q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdEsmnJgnHBF"
      },
      "outputs": [],
      "source": [
        "#Confusion matrix of Multinomial Naive Bayes model\n",
        "#  Compute the confusion matrix\n",
        "conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n",
        "\n",
        "# print the confusion matrix\n",
        "print(\"Confusion Matrix (Naive Bayes):\")\n",
        "print(conf_matrix_nb)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n",
        "\n",
        "# Plot the heatmap for the confusion matrix\n",
        "plt.figure(figsize=(4, 2))\n",
        "sns.heatmap(conf_matrix_nb, annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Confusion Matrix for Naive Bayes Model')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkzViUYnG5i5"
      },
      "outputs": [],
      "source": [
        "## Linear Support Vector Machine\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Train the LinearSVC model\n",
        "linear_svc_model = LinearSVC()\n",
        "linear_svc_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred_linear_svc = linear_svc_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy_linear_svc = accuracy_score(y_test, y_pred_linear_svc)\n",
        "print(f'Accuracy (LinearSVC): {accuracy_linear_svc}')\n",
        "print(classification_report(y_test, y_pred_linear_svc))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance Evaluation\n",
        "\n",
        "#Function to evaluate performance\n",
        "def evaluate_linear_svc(model, X_test, y_test):\n",
        "    # Start the timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate the evaluation runtime\n",
        "    eval_runtime = time.time() - start_time\n",
        "\n",
        "    # Calculate the number of samples processed per second\n",
        "    # Use .shape[0] to get the number of rows in X_test\n",
        "    eval_samples_per_second = X_test.shape[0] / eval_runtime\n",
        "\n",
        "    return {\n",
        "        'eval_runtime': eval_runtime,\n",
        "        'eval_samples_per_second': eval_samples_per_second,\n",
        "    }\n",
        "\n",
        "# Now, evaluate the LinearSVC model\n",
        "evaluation_results = evaluate_linear_svc(linear_svc_model, X_test, y_test)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(evaluation_results)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jj8TMMwO2tey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K9tR_WzwwCT"
      },
      "outputs": [],
      "source": [
        "#Confusion Matrix of LinearSVC\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix_linear_svc = confusion_matrix(y_test, y_pred_linear_svc)\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(4, 2))\n",
        "sns.heatmap(conf_matrix_linear_svc, annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Confusion Matrix for LinearSVC Model')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbiCoBczHBQQ"
      },
      "outputs": [],
      "source": [
        "##Random Forest Classifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Launch the Random Forest classification system\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f'Random Forest Accuracy: {accuracy_rf * 100:.2f}%')\n",
        "\n",
        "# Generate a classification report\n",
        "report_rf = classification_report(y_test, y_pred_rf)\n",
        "print(report_rf)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance Evaluation\n",
        "\n",
        "#function to evaluate performance of the model\n",
        "def evaluate_random_forest(model, X_test, y_test):\n",
        "    # Start the timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Predict probabilities\n",
        "    proba = model.predict_proba(X_test)\n",
        "\n",
        "    # Convert y_test to one-hot encoding if it's not already\n",
        "    y_test_one_hot = pd.get_dummies(y_test).values\n",
        "\n",
        "    # Calculate the evaluation runtime\n",
        "    eval_runtime = time.time() - start_time\n",
        "\n",
        "    # Calculate the number of samples processed per second\n",
        "    eval_samples_per_second = X_test.shape[0] / eval_runtime\n",
        "\n",
        "    # Calculate log loss (cross-entropy loss)\n",
        "    eval_loss = log_loss(y_test_one_hot, proba)\n",
        "\n",
        "    return {\n",
        "        'eval_loss': eval_loss,\n",
        "        'eval_runtime': eval_runtime,\n",
        "        'eval_samples_per_second': eval_samples_per_second,\n",
        "    }\n",
        "\n",
        "# Now, evaluate the Random Forest model\n",
        "evaluation_results = evaluate_random_forest(rf_classifier, X_test, y_test)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(evaluation_results)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tEmsk46IJz1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e44wXYPjq0Ku"
      },
      "outputs": [],
      "source": [
        "#Confusion matrix of RF model\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "# Step 2: Plot the heatmap\n",
        "plt.figure(figsize=(4, 2))\n",
        "sns.heatmap(conf_matrix_rf, annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Confusion Matrix for Random Forest Model')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jPV_AbeHGN9"
      },
      "outputs": [],
      "source": [
        "##DistilBERT Model\n",
        "\n",
        "#Setup and imports\n",
        "!pip install transformers\n",
        "!pip install accelerate -U\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ubhyIXhcRXZ"
      },
      "outputs": [],
      "source": [
        "#Data pre-processing and tokenization\n",
        "# Count the number of unique labels/classes in your dataset\n",
        "num_classes = tweets_df['labels'].nunique()\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_classes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qhlT37jgeb4F"
      },
      "outputs": [],
      "source": [
        "#Label Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a label encoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the label encoder to the  label data and transform the labels to integers\n",
        "tweets_df['encoded_labels'] = label_encoder.fit_transform(tweets_df['labels'])\n",
        "\n",
        "# Now the labels are encoded as integers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qsc8zvdme03v"
      },
      "outputs": [],
      "source": [
        "#Custom data set creation\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # Ensure labels are long type\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Using the new 'encoded_labels' for the dataset\n",
        "dataset = TweetDataset(tweets_df['cleaned_tweets'].tolist(), tweets_df['encoded_labels'].tolist(), tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uWBVZ1Bfe9Ek"
      },
      "outputs": [],
      "source": [
        "#splitting the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "juSUWD6bhA0v"
      },
      "outputs": [],
      "source": [
        "#Model training setup\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bmFaZN6OZt2"
      },
      "outputs": [],
      "source": [
        "#Model Evaluation\n",
        "trainer.evaluate()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Using the test dataset to get the predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Compute metrics\n",
        "metrics = compute_metrics(predictions)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Accuracy:\", metrics['accuracy'])\n",
        "print(\"Precision:\", metrics['precision'])\n",
        "print(\"Recall:\", metrics['recall'])\n",
        "print(\"F1-Score:\", metrics['f1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmIVChq2en-z"
      },
      "outputs": [],
      "source": [
        "#Saving the model\n",
        "model.save_pretrained('./my_model')\n",
        "tokenizer.save_pretrained('./my_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZLzq8F2zgLA"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix of DistilBERT\n",
        "\n",
        "# Extracting the actual labels and predicted labels\n",
        "actual_labels = predictions.label_ids\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "#  Create the confusion matrix\n",
        "conf_matrix_distilbert = confusion_matrix(actual_labels, predicted_labels)\n",
        "\n",
        "#  Plot the heatmap\n",
        "plt.figure(figsize=(4, 2))\n",
        "sns.heatmap(conf_matrix_distilbert, annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Confusion Matrix for DistilBERT Model')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}